import torch
import torch.nn as nn
import torch.utils.data as data
import torchvision
import torch.nn.functional as F
import torchvision.transforms as transforms
import torch.optim as optim
from torch.autograd import Variable

BATCH_SIZE = 128
NUM_EPOCHS = 10

normalize = transforms.Normalize(mean=[.5], std=[.5])
transform = transforms.Compose([transforms.ToTensor(), normalize])

# download and load the data
train_dataset = torchvision.datasets.MNIST(root='./mnist/', train=True, transform=transform, download=True)
test_dataset = torchvision.datasets.MNIST(root='./mnist/', train=False, transform=transform, download=False)

# encapsulate them into dataloader form
train_loader = data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)
test_loader = data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=True)


class SimpleNet(nn.Module):
    def __init__(self):
        super(SimpleNet, self).__init__()
        self.conv1 = nn.Sequential(nn.Conv2d(1, 8, 3, 1, 2), nn.ReLU(),nn.MaxPool2d(2, 2))
        self.conv2 = nn.Sequential(nn.Conv2d(8, 16, 5), nn.ReLU(),nn.MaxPool2d(2, 2))
        self.fc1 = nn.Sequential(nn.Linear(16 * 5 * 5, 256),nn.BatchNorm1d(256), nn.ReLU())
        self.fc2 = nn.Sequential(
            nn.Linear(256, 120),
            nn.BatchNorm1d(120),
            nn.ReLU(),
        )
        self.fc3 = nn.Sequential(
            nn.Linear(120, 84),
            nn.BatchNorm1d(84),
            nn.ReLU(),
            nn.Linear(84, 10)
        )
        self.dropout = nn.Dropout(0.23)      #使用dropout来防止过拟合
    def forward(self,x):
        x = self.conv1(x)
        x = self.conv2(x)
        x = x.view(x.size()[0], -1)
        x = self.fc1(x)
        x = self.dropout(x)
        x = self.fc2(x)
        x = self.dropout(x)
        x = self.fc3(x)
        return x

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = SimpleNet().to(device)
criterion = nn.CrossEntropyLoss()# 损失函数使用交叉熵
optimizer = optim.Adam(model.parameters(),lr = 0.0009)# 优化函数使用Adam算法


for epoch in range(NUM_EPOCHS):
    print("epoch:"+' ' +str(epoch))

    model.train()    #训练模式
    sum_loss = 0.0
    correct = 0
    total = 0
    for i,data in enumerate(train_loader):
        inputs, labels = data
        inputs, labels = Variable(inputs).cuda(), Variable(labels).cuda()
        optimizer.zero_grad()  #将梯度归零
        outputs = model(inputs)  #前向运算
        loss = criterion(outputs, labels)  #损失函数
        loss.backward()  #反向传播
        optimizer.step()  #做一步参数更新
        _,pred = torch.max(outputs, 1)
        num_correct = (pred == labels).sum()
        total += labels.size(0)
        correct += (pred == labels).sum()

    print("Train accuracy: ",(correct.item() /len(train_dataset))) #输出结果

    model.eval()  #测试模式
    correct = 0
    total = 0
    for data_test in test_loader:
        inputs, labels = data_test
        inputs, labels = Variable(inputs).cuda(), Variable(labels).cuda()
        labels = Variable(labels)
        testout = model(inputs)
        testloss = criterion(testout,labels)
        _,pred = torch.max(testout, 1)
        num_correct = (pred == labels).sum()
        total += labels.size(0)
        correct += (pred == labels).sum()

    print("Test accuracy: ",(correct.item() /len(test_dataset)))  #输出结果
